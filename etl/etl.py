import os
import pandas as pd
from datetime import datetime

def getUrls() -> list:
    """
    Gera a lista completa de URLs dos arquivos CSV do conjunto 
    ‚ÄúVoo Regular Ativo (VRA)‚Äù disponibilizado pela ANAC.

    A fun√ß√£o constr√≥i dinamicamente os caminhos de acesso aos arquivos 
    organizados por ano e m√™s, conforme a estrutura oficial do portal de 
    dados abertos da ANAC. S√£o consideradas todas as combina√ß√µes entre os anos 
    de 2000 a 2025 e os 12 meses do ano, com exce√ß√£o dos meses posteriores a 
    outubro de 2025, pois esses arquivos ainda n√£o est√£o dispon√≠veis.

    Para cada combina√ß√£o v√°lida, √© gerada a URL correspondente no formato:
        https://.../ANO/MM - M√™s/VRA_ANOMM.csv

    Retorna uma lista contendo todas as URLs resultantes, na ordem cronol√≥gica.

    Retorno
    -------
    list
        Lista de strings contendo as URLs completas dos arquivos CSV do VRA.
    """

    url_base = "https://sistemas.anac.gov.br/dadosabertos/Voos%20e%20opera%C3%A7%C3%B5es%20a%C3%A9reas/Voo%20Regular%20Ativo%20%28VRA%29"
    anos = list(range(2000, 2026))
    meses = {
        1:  "01%20-%20Janeiro",
        2:  "02%20-%20Fevereiro",
        3:  "03%20-%20Mar%C3%A7o",
        4:  "04%20-%20Abril",
        5:  "05%20-%20Maio",
        6:  "06%20-%20Junho",
        7:  "07%20-%20Julho",
        8:  "08%20-%20Agosto",
        9:  "09%20-%20Setembro",
        10: "10%20-%20Outubro",
        11: "11%20-%20Novembro",
        12: "12%20-%20Dezembro"
    }

    urls = []

    for ano in anos:
        for mes in meses.items():
            if ano == 2025 and mes[0] > 10:
                continue
            url = f"{url_base}/{ano}/{mes[1]}/VRA_{ano}{mes[0]}.csv"
            urls.append(url)
    
    return urls


def preprocess_csvs(urls: list) -> pd.DataFrame:
    """
    Carrega, filtra e preprocessa m√∫ltiplos arquivos CSV do VRA (Voo Regular Ativo) 
    disponibilizados pela ANAC, retornando um √∫nico DataFrame consolidado.

    Este procedimento realiza o download sequencial dos arquivos, aplica regras de ETL 
    para limpeza e normaliza√ß√£o dos dados e concatena os resultados em um DataFrame √∫nico. 
    Ao final, os dados retornados cont√™m apenas informa√ß√µes relevantes para an√°lise de 
    voos realizados, com colunas categ√≥ricas otimizadas e datas convertidas para datetime.

    Etapas executadas para cada arquivo CSV:
        1. Download e leitura do arquivo bruto, ignorando as duas primeiras linhas 
           (‚ÄúAtualizado em‚Äù e o cabe√ßalho original).
        2. Sele√ß√£o das colunas necess√°rias conforme o layout oficial da ANAC.
        3. Remo√ß√£o de linhas cuja situa√ß√£o do voo seja "CANCELADO".
        4. Remo√ß√£o de registros com valores ausentes nas colunas de data/hora.
        5. Convers√£o das colunas de data para o tipo datetime.
        6. Convers√£o de colunas categ√≥ricas para o tipo category (otimiza√ß√£o de mem√≥ria).
        7. Concatena√ß√£o incremental no DataFrame mestre.

    Par√¢metros
    ----------
    urls : list
        Lista contendo as URLs dos arquivos CSV a serem processados.

    Retorno
    -------
    pandas.DataFrame
        DataFrame consolidado contendo apenas voos realizados e colunas previamente 
        definidas, com tipagem otimizada e datas convertidas.

    Observa√ß√µes
    -----------
    - A fun√ß√£o imprime estat√≠sticas de progresso, quantidade de linhas carregadas 
      e uso de mem√≥ria ao longo do processo.
    """

    print(f"Iniciando o download e preprocessamento de {len(urls)} arquivos CSV...\n")

    # Fun√ß√£o interna para limpeza do DataFrame
    def clean_df(df: pd.DataFrame, columns: list) -> pd.DataFrame:
        # Remove voos cancelados
        df = df[df["Situa√ß√£o Voo"] == "REALIZADO"]

        # Remove NaN das colunas "Partida Prevista" e "Partida Real"
        df = df.dropna(subset=["Partida Prevista", "Partida Real"])

        # Mant√©m apenas as colunas desejadas
        df = df[columns]

        return df
    
    # Fun√ß√£o interna para converter colunas categ√≥ricas
    def parse_categoricals(df: pd.DataFrame) -> pd.DataFrame:
        categorical_columns = [
            "Empresa A√©rea",
            "C√≥digo Tipo Linha",
            "Aer√≥dromo Origem",
            "Aer√≥dromo Destino",
        ]
        for col in categorical_columns:
            df[col] = df[col].astype('category')
        
        return df
    
    # Fun√ß√£o interna para converter colunas de data/hora
    def parse_datetime(df: pd.DataFrame) -> pd.DataFrame:
        datetime_columns = [
            "Partida Prevista",
            "Partida Real",
        ]
        for col in datetime_columns:
            df[col] = pd.to_datetime(df[col], format="mixed", dayfirst=True, errors='coerce')
        
        return df
    
    # Colunas finais desejadas
    columns = [
        "Empresa A√©rea",
        "C√≥digo Tipo Linha",
        "Aer√≥dromo Origem",
        "Aer√≥dromo Destino",
        "Partida Prevista",
        "Partida Real",
    ]

    # Colunas do CSV original disponibilizado pela ANAC
    raw_columns = [
        "Empresa A√©rea",
        "N√∫mero Voo",
        "C√≥digo Autoriza√ß√£o (DI)",
        "C√≥digo Tipo Linha",
        "Aer√≥dromo Origem",
        "Aer√≥dromo Destino",
        "Partida Prevista",
        "Partida Real",
        "Chegada Prevista",
        "Chegada Real",
        "Situa√ß√£o Voo",
        "C√≥digo Justificativa"
    ]

    # Inicializa o DataFrame mestre apenas com as colunas desejadas
    master_df = pd.DataFrame(columns=columns)

    # Inicializa lista para armazenar DataFrames individuais
    dfs = []

    # Vari√°veis de controle
    lines = 0
    memory_usage = 0

    for i, url in enumerate(urls, start=1):

        print(f"[{i}/{len(urls)}] Carregando: {url.replace('https://sistemas.anac.gov.br/dadosabertos/Voos%20e%20opera%C3%A7%C3%B5es%20a%C3%A9reas/Voo%20Regular%20Ativo%20%28VRA%29', 'http://...')}")

        try:
            # Leitura do CSV bruto
            df = pd.read_csv(
                url,
                sep=';',
                quotechar='"',
                skiprows=2,         # pula "Atualizado em" + header
                header=None,
                names=raw_columns,
                low_memory=False
            )

        except Exception as e:
            print(f"‚ùå Falha ao ler {url}\nErro: {e}")
            continue

        if df.empty:
            print(f"‚ö†Ô∏è CSV vazio em {url}, ignorando.")
            continue

        # Limpeza e parseamento
        df = clean_df(df, columns)
        df = parse_categoricals(df)
        df = parse_datetime(df)

        # Adiciona o df limpo √† lista
        dfs.append(df)

        lines += df.shape[0]
        memory_usage += df.memory_usage(deep=True).sum() / (1024 ** 2)
        print(f"‚úî {df.shape[0]} linhas carregadas.")
        print(f"   Total atual de linhas: {lines}")
        print(f"   Mem√≥ria usada: {memory_usage:.2f} MB\n")

    # Concatena todos os dfs ao DataFrame mestre
    master_df = pd.concat(dfs, ignore_index=True)

    print(f"\nüèÅ Finalizado.\n")
    print(f"Total de linhas carregadas: {master_df.shape[0]}")
    print(f"Mem√≥ria usada no Dataframe Master: {master_df.memory_usage(deep=True).sum() / (1024 ** 2):.2f} MB\n")

    return master_df


def save_df(df: pd.DataFrame, filename: str = "vra_master", timestamp: bool = False) -> None:
    """
    Salva o DataFrame em formatos CSV e Parquet dentro do diret√≥rio root/data/.

    Par√¢metros
    ----------
    df : pd.DataFrame
        DataFrame que ser√° salvo.
    filename : str, opcional
        Nome base do arquivo (sem extens√£o). O padr√£o √© "vra_master".
    timestamp : bool, opcional
        Se True, adiciona ao nome do arquivo um sufixo com data e hora
        no formato YYYYMMDD_HHMMSS, garantindo unicidade e versionamento.

    Notas
    -----
    - O diret√≥rio root/data/ √© criado automaticamente caso n√£o exista.
    - Dois arquivos s√£o gerados:
        ‚Ä¢ <filename>.csv (codifica√ß√£o UTF-8)  
        ‚Ä¢ <filename>.parquet (colunar, compactado)
    - O Parquet √© recomendado para processamento posterior devido √† maior velocidade
      de leitura e economia de mem√≥ria.
    """

    # Garante que o diret√≥rio ./data/ exista
    data_dir = os.path.join(os.path.dirname(__file__), "..", "data")
    data_dir = os.path.abspath(data_dir)
    os.makedirs(data_dir, exist_ok=True)

    # Se timestamp=True, adiciona YYYYMMDD_HHMMSS ao nome do arquivo
    filename_raw = filename
    if timestamp:
        base, ext = os.path.splitext(filename)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{base}_{ts}{ext}"
        filename_raw = f"{base}_{ts}"

    # Caminho completo para salvar o arquivo
    filepath = os.path.join(data_dir, filename)

    # Salva o DataFrame em CSV
    df.to_csv(f'{filepath}.csv', index=False, encoding="utf-8")
    df.to_parquet(f'{filepath}.parquet', index=False)

    print(f"üìÅ Arquivos salvos com sucesso:")
    print(f"   ‚Üí ./data/{filename_raw}.csv")
    print(f"   ‚Üí ./data/{filename_raw}.parquet")

def main() -> None:
    '''
    Fun√ß√£o principal para executar o processo ETL completo direto da linha de comando.
    '''
    urls = getUrls()
    master_dataframe = preprocess_csvs(urls)
    save_df(master_dataframe, timestamp=True)

    master_dataframe.info()
    master_dataframe.head()

if __name__ == "__main__":
    main()